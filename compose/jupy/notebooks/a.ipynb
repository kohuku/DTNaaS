{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "from prometheus_http_client import Prometheus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from http.client import HTTPException\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTN(object):\n",
    "    def __init__(self, name, man_addr, data_addr, mon_addr, interface):\n",
    "        self.name = name\n",
    "        self.man_addr = man_addr\n",
    "        self.data_addr = data_addr\n",
    "        self.interface = interface\n",
    "        self.mon_addr = mon_addr\n",
    "\n",
    "# register DTN to orchestrator\n",
    "def add_dtn_to_orchestrator(sender, receiver, orchestrator):\n",
    "\n",
    "    data = {\n",
    "        'name': 'receiver',\n",
    "        'man_addr': receiver.man_addr,\n",
    "        'data_addr': receiver.data_addr,\n",
    "        'username': 'nobody',\n",
    "        'interface': receiver.interface\n",
    "    }\n",
    "    # register receiver and get ID 1\n",
    "    response = requests.get('{}'.format(orchestrator))\n",
    "    print(response)\n",
    "    response = requests.post('{}/DTN/'.format(orchestrator), json=data)    \n",
    "    result = response.json()\n",
    "    print('receiver : ',result)\n",
    "    #assert result == {'id': 8}\n",
    "    \n",
    "    data = {\n",
    "        'name': 'sender',\n",
    "        'man_addr': sender.man_addr,\n",
    "        'data_addr': sender.data_addr,\n",
    "        'username': 'nobody',        \n",
    "        'interface': sender.interface\n",
    "    }\n",
    "    # register sender and get ID 2\n",
    "    response = requests.post('{}/DTN/'.format(orchestrator), json=data)\n",
    "    result = response.json()\n",
    "    print('sender : ',result)\n",
    "    #assert result == {'id': 9}\n",
    "    \n",
    "\n",
    "\n",
    "receiver = DTN('192.168.10.141','192.168.10.141:5000', '192.168.10.141', '192.168.10.141:5000',  'enp1s0')\n",
    "sender = DTN('192.168.10.113', '192.168.10.113:5000', '192.168.10.113', '192.168.10.113:5000', 'enp1s0')\n",
    "orchestrator = 'http://192.168.10.113:5002'\n",
    "monitor = 'http://192.168.10.156:9090'\n",
    "\n",
    "# DTNs already added\n",
    "#add_dtn_to_orchestrator(sender,receiver,orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_INT = 3\n",
    "STEP = 15\n",
    "MAX_RES = 11000\n",
    "\n",
    "\n",
    "# check latency between DTNs\n",
    "def test_ping(orchestrator):\n",
    "    response = requests.get('{}/ping/4/3'.format(orchestrator))\n",
    "    result = response.json()\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Start Transfer from sender to receiver on /data directory\n",
    "def test_transfer(sender, receiver, orchestrator, num_workers = 1):   \n",
    "    # retrieve files from sender \n",
    "    result =  requests.get('http://{}/files/'.format(sender.man_addr))    \n",
    "\n",
    "    # classify files and dirs from returned list\n",
    "    files = result.json()\n",
    "    file_list = [i['name'] for i in files if i['type'] == 'file'][:5]\n",
    "    dirs = [i['name'] for i in files if i['type'] == 'dir']\n",
    "    print(file_list,dirs)\n",
    "\n",
    "    # create dirs in receiver\n",
    "    response = requests.post('http://{}/create_dir/'.format(receiver.man_addr),json=dirs)\n",
    "    if response.status_code != 200: raise Exception('failed to create dirs')\n",
    "\n",
    "    # prepare files to send \n",
    "    data = {\n",
    "        # list of files to send\n",
    "        'srcfile' : file_list,\n",
    "        # list of files to receive\n",
    "        'dstfile' : file_list,\n",
    "        # number of simultaneous connAVG_INTections\n",
    "        'num_workers' : num_workers,\n",
    "        # block size to use\n",
    "        'blocksize' : 1024,\n",
    "        # disable zero-copy\n",
    "        'zerocopy' : False,\n",
    "    }\n",
    "\n",
    "    # start transfer using nuttcp\n",
    "    response = requests.post('{}/transfer/nuttcp/4/3'.format(orchestrator),json=data) \n",
    "    result = response.json()\n",
    "    #print(result)\n",
    "    assert result['result'] == True\n",
    "    transfer_id = result['transfer']\n",
    "    print('transfer_id %s' % (transfer_id))\n",
    "    return transfer_id\n",
    "\n",
    "\n",
    "# clean up DTNs after transfer\n",
    "def cleanup(sender, receiver, retry = 5):\n",
    "\n",
    "    for i in range(0, retry):        \n",
    "        response = requests.get('http://{}/cleanup/nuttcp'.format(sender.man_addr))\n",
    "        if response.status_code != 200: continue\n",
    "        response = requests.get('http://{}/cleanup/nuttcp'.format(receiver.man_addr))\n",
    "        if response.status_code != 200: continue        \n",
    "        \n",
    "        return \n",
    "    raise Exception('Cannot cleanup after %s tries' % retry)\n",
    "\n",
    "# wait for transfer to finish\n",
    "def wait_for_transfer(transfer_id, orchestrator, sender):\n",
    "    while True:\n",
    "        response = requests.get('{}/check/{}'.format(orchestrator, transfer_id))\n",
    "        result = response.json()\n",
    "        print(result) # {finished : ? , unfinished : ?}\n",
    "        if result['Unfinished'] == 0:\n",
    "            response = requests.get('http://{}/cleanup/nuttcp'.format(sender.man_addr))\n",
    "            print(\"clean up\",response.json())\n",
    "            break\n",
    "        time.sleep(3)\n",
    "\n",
    "# mark transfer to finished\n",
    "def finish_transfer(transfer_id, orchestrator, sender, receiver):    \n",
    "    response = requests.post('{}/wait/{}'.format(orchestrator, transfer_id))\n",
    "    result = response.json()\n",
    "    print(\"finish_transfer result\",result)\n",
    "    cleanup(sender, receiver)\n",
    "\n",
    "# get transfer detail\n",
    "def get_transfer(transfer_id, orchestrator):\n",
    "    \n",
    "    response = requests.get('{}/transfer/{}'.format(orchestrator, transfer_id))\n",
    "    result = response.json()\n",
    "    #print(result)\n",
    "    return result\n",
    "\n",
    "# send data query to DTNs\n",
    "def send_query(query, start, end, step, url):\n",
    "    \n",
    "    prometheus = Prometheus()\n",
    "    prometheus.url = url\n",
    "\n",
    "    res = prometheus.query_rang(metric=query, start=start, end=end, step=step)    \n",
    "    return res\n",
    "\n",
    "# remove unnecessary header for dataset\n",
    "def prettify_header(metric):\n",
    "    metrics_to_remove = ['instance', 'job', 'mode', '__name__', 'container', 'endpoint', 'namespace', 'pod', 'prometheus', 'service']\n",
    "    for i in metrics_to_remove:\n",
    "        if i in metric: del metric[i]\n",
    "    if len(metric) > 1 : raise Exception('too many metric labels')\n",
    "    else:\n",
    "        return next(iter(metric.keys()))\n",
    "\n",
    "# extract data from monitoring system\n",
    "def extractor(sender, receiver, start_time, end_time, monitor_url):\n",
    "    AVG_INT = 3        \n",
    "    query = (\n",
    "    'label_replace(sum by (instance)(irate(node_network_transmit_bytes_total{{instance=~\"{4}.*\", device=\"{2}\"}}[{1}m])), \"network_throughput\", \"$0\", \"instance\", \"(.+)\") '\n",
    "    'or label_replace(sum by (job)(irate(node_disk_written_bytes_total{{instance=~\"{5}.*\", device=~\"nvme.*\"}}[{1}m])),\"Goodput\", \"$0\", \"job\", \"(.+)\") '        \n",
    "    'or label_replace(sum by (job)(1 - irate(node_cpu_seconds_total{{mode=\"idle\", instance=\"{4}\"}}[1m])),\"CPU\", \"$0\", \"job\", \"(.+)\") '\n",
    "    'or label_replace(max by (container)(container_memory_working_set_bytes{{namespace=\"{3}\", container=~\"{0}.*\"}}), \"Memory_used\", \"$0\", \"container\", \"(.+)\") '\n",
    "    'or label_replace(node_memory_Active_bytes{{instance=\"{4}\"}}, \"Memory_used\", \"$0\", \"instance\", \"(.+)\") '    \n",
    "    'or label_replace(sum by (job)(irate(node_disk_read_bytes_total{{instance=~\"{4}.*\", device=~\"nvme.*\"}}[{1}m])),\"NVMe_transfer_bytes\", \"$0\", \"job\", \"(.+)\") '\n",
    "    'or label_replace(sum by (job)(irate(node_disk_io_time_seconds_total{{instance=~\"{4}.*\", device=~\"nvme.*\"}}[{1}m])),\"NVMe_total_util\", \"$0\", \"job\", \"(.+)\") '    \n",
    "    'or label_replace(count by (job)(node_disk_io_time_seconds_total{{instance=~\"{4}.*\", device=~\"nvme[0-7]n1\"}}),\"Storage_count\", \"$0\", \"job\", \"(.+)\") '\n",
    "    'or label_replace(sum by (job)(node_network_speed_bytes{{instance=~\"{4}.*\", device=\"{2}\"}} * 8), \"NIC_speed\", \"$0\", \"job\", \"(.+)\") '\n",
    "    'or label_replace(sum by (job)(irate(node_netstat_Tcp_RetransSegs{{instance=~\"{4}.*\"}}[{1}m])), \"Packet_losses\", \"$0\", \"job\", \"(.+)\") '\n",
    "    '').format(sender.name, AVG_INT, sender.interface, 'dtnaas', sender.mon_addr, receiver.mon_addr)\n",
    "\n",
    "    dataset = None\n",
    "    \n",
    "    while end_time > start_time:        \n",
    "        data_in_period = None\n",
    "        max_ts = start_time + (STEP * MAX_RES) \n",
    "        next_hop_ts = end_time if max_ts > end_time else max_ts\n",
    "        logging.debug('Getting data for {} : {}'.format(start_time, end_time))\n",
    "        res = send_query(query, start_time, next_hop_ts, STEP, monitor_url)\n",
    "        print(res)\n",
    "        if '401 Authorization Required' in res: raise HTTPException(res)\n",
    "        response = json.loads(res)\n",
    "        if response['status'] != 'success': raise Exception('Failed to query Prometheus server')\n",
    "        #print(response)\n",
    "        for result in response['data']['result']:\n",
    "            result['metric'] = prettify_header(result['metric'])            \n",
    "            df = pd.DataFrame(data=result['values'], columns = ['Time', result['metric']], dtype=float)            \n",
    "            df['Time'] = pd.to_datetime(df['Time'], unit='s')\n",
    "            df.set_index('Time', inplace=True)\n",
    "\n",
    "            data_in_period = df if data_in_period is None else data_in_period.merge(df, how='outer',  on='Time').sort_index()\n",
    "        \n",
    "        dataset = data_in_period if dataset is None else dataset.append(data_in_period)\n",
    "        start_time = next_hop_ts\n",
    "\n",
    "    cols = dataset.columns.tolist()\n",
    "    labels_to_rearrange = ['NVMe_total_util', 'NVMe_transfer_bytes']    \n",
    "    for i in labels_to_rearrange: \n",
    "        cols.remove(i)\n",
    "        cols.insert(0,i)    \n",
    "    \n",
    "    return dataset[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mtime': 1695307678.8601706, 'name': 'test', 'size': 4, 'type': 'file'}, {'mtime': 1695310305.3546433, 'name': 'dummy1G.txt', 'size': 10737418240, 'type': 'file'}, {'mtime': 1695621263.6084392, 'name': 'dummy1G_2.txt', 'size': 10737418240, 'type': 'file'}, {'mtime': 1695620923.5662122, 'name': 'dummy1G1.txt', 'size': 10737418240, 'type': 'file'}, {'mtime': 1695621298.5287223, 'name': 'dummy1G_4.txt', 'size': 10737418240, 'type': 'file'}, {'mtime': 1695621288.1966395, 'name': 'dummy1G_3.txt', 'size': 10737418240, 'type': 'file'}]\n",
      "['test', 'dummy1G.txt', 'dummy1G_2.txt', 'dummy1G1.txt', 'dummy1G_4.txt'] []\n"
     ]
    }
   ],
   "source": [
    "print(requests.get('http://{}/files/'.format(sender.man_addr)).json())\n",
    "\n",
    "result =  requests.get('http://{}/files/'.format(sender.man_addr))    \n",
    "\n",
    "# classify files and dirs from returned list\n",
    "files = result.json()\n",
    "file_list = [i['name'] for i in files if i['type'] == 'file'][:5]\n",
    "dirs = [i['name'] for i in files if i['type'] == 'dir']\n",
    "print(file_list,dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Ping ( orchestrator )\n",
      "{'latency': 0.0006442070007324219}\n",
      "Start Transfer\n",
      "['test', 'dummy1G.txt', 'dummy1G_2.txt', 'dummy1G1.txt', 'dummy1G_4.txt'] []\n",
      "transfer_id 42\n",
      "42\n",
      " waiting for transfer to finish\n",
      "{'Finished': 0, 'Unfinished': 5}\n",
      "{'Finished': 0, 'Unfinished': 5}\n",
      "{'Finished': 1, 'Unfinished': 4}\n",
      "{'Finished': 1, 'Unfinished': 4}\n",
      "{'Finished': 2, 'Unfinished': 3}\n",
      "{'Finished': 2, 'Unfinished': 3}\n",
      "{'Finished': 3, 'Unfinished': 2}\n",
      "{'Finished': 3, 'Unfinished': 2}\n",
      "{'Finished': 4, 'Unfinished': 1}\n",
      "{'Finished': 5, 'Unfinished': 0}\n",
      "clean up null\n",
      "\n",
      "Transfer finished\n",
      "finish_transfer result {'failed': ['dummy1G.txt', 'dummy1G_4.txt', 'dummy1G1.txt', 'dummy1G_2.txt', 'test'], 'result': True}\n",
      "transfer_detail {'message': 'Need to wait for the transfer id 42'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtransfer_detail\u001b[39m\u001b[39m\"\u001b[39m,transfer_detail)\n\u001b[1;32m     17\u001b[0m \u001b[39m# use the transfer detail to query dataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df \u001b[39m=\u001b[39m extractor(sender, receiver, transfer_detail[\u001b[39m'\u001b[39;49m\u001b[39mstart_time\u001b[39;49m\u001b[39m'\u001b[39;49m], transfer_detail[\u001b[39m'\u001b[39m\u001b[39mend_time\u001b[39m\u001b[39m'\u001b[39m], monitor)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_time'"
     ]
    }
   ],
   "source": [
    "print(\"Test Ping ( orchestrator )\")\n",
    "test_ping(orchestrator)\n",
    "\n",
    "print (\"Start Transfer\")\n",
    "# start transfer\n",
    "transfer_id = test_transfer(sender, receiver, orchestrator,2)\n",
    "print(transfer_id)\n",
    "print(\" waiting for transfer to finish\")\n",
    "wait_for_transfer(transfer_id, orchestrator, sender)\n",
    "print(\"Transfer finished\")\n",
    "# mark transfer to finished\n",
    "finish_transfer(transfer_id, orchestrator, sender, receiver)\n",
    "\n",
    "# get transfer detail\n",
    "transfer_detail = get_transfer(transfer_id, orchestrator)\n",
    "print(\"transfer_detail\",transfer_detail)\n",
    "# use the transfer detail to query dataset\n",
    "df = extractor(sender, receiver, transfer_detail['start_time'], transfer_detail['end_time'], monitor)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
